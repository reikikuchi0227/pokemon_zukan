# ---------------------------------------------------------------
# 0.å„ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
# ---------------------------------------------------------------
import os
# ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆã—ã¦é‡è¤‡åˆ¤å®šã«ä½¿ç”¨ã™ã‚‹
import hashlib
# ç”»åƒã‚’èª­ã¿è¾¼ã‚“ã§ãƒãƒƒã‚·ãƒ¥è¨ˆç®—ç”¨ã«æ•´å½¢
from PIL import Image
# Googleç”»åƒæ¤œç´¢ã§è‡ªå‹•åé›†ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from icrawler.builtin import GoogleImageCrawler
# ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ä¸€æ„ã«ã™ã‚‹ãŸã‚ã«è¿½åŠ 
import uuid

import shutil
    

# ---------------------------------------------------------------
# 1.ãƒãƒƒã‚·ãƒ¥ã®å–å¾—é–¢æ•°
# ---------------------------------------------------------------
def get_image_hash(image_path):
    # ç”»åƒã®MD5ãƒãƒƒã‚·ãƒ¥ã‚’ä½œæˆãƒ»å–å¾—
    # â†’åŒã˜ç”»åƒã¯åŒã˜ãƒãƒƒã‚·ãƒ¥å€¤ã«ãªã‚‹
    try:
        with Image.open(image_path) as img:
            # ç”»åƒã‚’64Ã—64ã«ãƒªã‚µã‚¤ã‚ºã€RGBã«å¤‰æ›
            img = img.resize((64, 64)).convert("RGB")
            return hashlib.md5(img.tobytes()).hexdigest()
    except Exception:
        return None
    

# ---------------------------------------------------------------
# 2.ä¿å­˜æ¸ˆã¿ã®ç”»åƒã®ãƒãƒƒã‚·ãƒ¥ä¸€è¦§å–å¾—
# ---------------------------------------------------------------
def get_existing_hashes(folder_path):
    # å–å¾—ã—ãŸã™ã¹ã¦ã®ãƒãƒƒã‚·ãƒ¥ã‚’setã«æ ¼ç´
    hashes = set()
    for fname in os.listdir(folder_path):
        path = os.path.join(folder_path, fname)
        if os.path.isfile(path):
            h = get_image_hash(path)
            if h:
                hashes.add(h)
    return hashes
    

# ---------------------------------------------------------------
# 3.ä¸€æ™‚ãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªç”»åƒã®ã¿ä¿å­˜å…ˆã«ç§»å‹•
# ---------------------------------------------------------------
def save_unique_images_from_temp(temp_dir, target_dir, existing_hashes, max_total):
    new_count = 0
    # temp_dirã«ä¿å­˜ã•ã‚ŒãŸç”»åƒã‚’ã™ã¹ã¦ãƒã‚§ãƒƒã‚¯
    for fname in os.listdir(temp_dir):
        if len(existing_hashes) >= max_total:
            break  # æ—¢ã«æœ€å¤§ã«é”ã—ã¦ã„ã‚Œã°çµ‚äº†
        
        fpath = os.path.join(temp_dir, fname)
        h = get_image_hash(fpath)
        # ãƒãƒƒã‚·ãƒ¥ãŒé‡è¤‡ã—ã¦ã„ãªã‘ã‚Œã°â†’target_dirã«ç§»å‹•
        if h and h not in existing_hashes:
            # ãƒ•ã‚¡ã‚¤ãƒ«åã®è¡çªã‚’é¿ã‘ã‚‹ãŸã‚ã«UUIDã‚’ä½¿ã£ã¦ãƒªãƒãƒ¼ãƒ 
            ext = os.path.splitext(fname)[1]
            unique_name = f"{uuid.uuid4().hex}{ext}"
            new_path = os.path.join(target_dir, unique_name)
            os.rename(fpath, new_path)
            
            existing_hashes.add(h)
            new_count += 1
        # é‡è¤‡ã—ã¦ã„ã‚Œã°å‰Šé™¤
        else:
            os.remove(fpath)
    # æ–°ã—ãè¿½åŠ ã•ã‚ŒãŸç”»åƒã®æšæ•°ã‚’è¿”ã™
    return new_count


# ---------------------------------------------------------------
# 4.é‡è¤‡ç”»åƒã‚’é™¤å¤–ã—ã¦è¤‡æ•°ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰200æšç”»åƒåé›†
# ---------------------------------------------------------------
def crawl_images_no_duplicates(keywords, target_dir, max_total=200, max_per_keyword=50):
    # terget_dirãŒãªã‘ã‚Œã°ä½œæˆ
    os.makedirs(target_dir, exist_ok=True)
    # æ—¢ã«ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ç”»åƒã®ãƒãƒƒã‚·ãƒ¥ä¸€è¦§ã‚’å–å¾—
    existing_hashes = get_existing_hashes(target_dir)
    
    # å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«å¯¾ã—ã¦é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ã«ä¸€æ™‚ãƒ•ã‚©ãƒ«ãƒ€ã‚’æº–å‚™
    for keyword in keywords:
        if len(existing_hashes) >= max_total:
            print("âœ… åé›†å®Œäº†ï¼š200æšã«åˆ°é”ã—ã¾ã—ãŸ")
            break
        
        print(f"æ¤œç´¢ä¸­: {keyword}")
        temp_dir = os.path.join(target_dir, "_temp")
        os.makedirs(temp_dir, exist_ok=True)
        
        # icrawerã‚’ä½¿ã£ã¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã§ç”»åƒã‚’temp_dirã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        crawler = GoogleImageCrawler(storage={"root_dir": temp_dir})
        crawler.crawl(keyword=keyword, max_num=max_per_keyword)
        
        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸç”»åƒã‹ã‚‰é‡è¤‡ãªã—ç”»åƒã‚’æœ¬ä¿å­˜
        added = save_unique_images_from_temp(temp_dir, target_dir, existing_hashes, max_total)
        print(f"{added}æšè¿½åŠ ã•ã‚Œã¾ã—ãŸ ({keyword})  ç¾åœ¨ã®åˆè¨ˆï¼š {len(existing_hashes)}")
    
        # æœ€å¾Œã«ä¸€æ™‚ãƒ•ã‚©ãƒ«ãƒ€_tempã‚’å‰Šé™¤
        # os.rmdir(temp_dir)
        shutil.rmtree(temp_dir, ignore_errors=True)

    print(f"ğŸ‰ æœ€çµ‚çš„ã«ä¿å­˜ã•ã‚ŒãŸç”»åƒæšæ•°ï¼š {len(existing_hashes)}")


# ---------------------------------------------------------------
# 5.ä½¿ç”¨
# ---------------------------------------------------------------
if __name__ == "__main__":
    keywords = ['ã‚¤ãƒ¼ãƒ–ã‚¤','ã‚¤ãƒ¼ãƒ–ã‚¤ ã¬ã„ãã‚‹ã¿', 'ã‚¤ãƒ¼ãƒ–ã‚¤ ã‚¢ãƒ‹ãƒ¡', 'ã‚¤ãƒ¼ãƒ–ã‚¤ ãƒ•ã‚£ã‚®ãƒ¥ã‚¢', 'ã‚¤ãƒ¼ãƒ–ã‚¤ ã‚²ãƒ¼ãƒ ', 'ã‚¤ãƒ¼ãƒ–ã‚¤ ã‚¤ãƒ©ã‚¹ãƒˆ']
    target_dir = r"C:\Users\harap\OneDrive\ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—\pokemon-main\pokemon_dataset\img_All\133_Eevee"
    crawl_images_no_duplicates(keywords, target_dir, max_total=200, max_per_keyword=50)